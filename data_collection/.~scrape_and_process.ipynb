{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab818636-1829-46dc-afdd-c43a95b7ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For performing regex operations\n",
    "import re\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from limitless_scrape import *\n",
    "from limitless_analysis import *\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10abc1b-ae1a-45dc-827f-b3868c566952",
   "metadata": {},
   "source": [
    "1. Create DataFrame that contains dates and URLs for each tournament in the Late Night's organizer page. \n",
    "2. Use checkpoint to compare which tournaments have already been scraped. \n",
    "3. Filter the DataFrame so that it only contains tournaments that have not been scraped. \n",
    "4. Put all URLs of filtered DataFrame to a list. \n",
    "5. Pass list to create_urls to create url_dict. \n",
    "6. Scrape all the URLs in url_dict. \n",
    "7. Process data so it looks like results\n",
    "8. Update Checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c59ac28-fd1e-41f1-81ab-ed7a1b13cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use checkpoint or scrape everything?\n",
    "use_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72167e7a-011d-4c83-aa45-50beefa5a8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight38/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight37/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight36/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight35/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight34/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight33/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight32/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/latenight31/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/61c4705854b82a0ab761b7d3/standings\n",
      "INFO:root:Scraping players table for https://play.limitlesstcg.com/tournament/61c46f3954b82a0ab761b7d2/standings\n",
      "INFO:root:Saving results to 'latest' folder...\n",
      "INFO:root:Saving results to 'dated' folder...\n",
      "INFO:root:Saving checkpoint to 'latest' folder...\n",
      "INFO:root:Saving checkpoint to 'dated' folder...\n"
     ]
    }
   ],
   "source": [
    "# 1. Create DataFrame that contans dates and URLS for each tournament\n",
    "df_latenight = scrape_for_dates_and_url()\n",
    "\n",
    "# 2. Use checkpoint. If not using checkpoint, scrape everything?\n",
    "if use_checkpoint:\n",
    "    ckpt_df = pd.read_csv(\"checkpoint/latest/checkpoint.csv\")\n",
    "    current_results_df = pd.read_csv(\"results/latest/scrape_results.csv\")\n",
    "    \n",
    "    # 3. and 4. Instead of filtering the DataFrame, we can use list comprehension to find net new urls. \n",
    "    ckpt_url_ls = ckpt_df['url'].unique().tolist()\n",
    "    all_url_ls = df_latenight['URL'].unique().tolist()\n",
    "    net_new_url_ls = [url for url in all_url_ls if url not in ckpt_url_ls] \n",
    "    \n",
    "    # 5. Create url dict; Only use first 2 urls as a test\n",
    "    url_dict = create_urls(net_new_url_ls[:10])\n",
    "                       \n",
    "    # # 6. Scrape urls in dict and add date\n",
    "    scrape_results_dict = multi_latenight_scrape(url_dict)\n",
    "    scrape_results_dict = add_date_to_dict(scrape_results_dict, df_latenight)\n",
    "    \n",
    "    # 7. Process data: Get WLT counts for each deck in each tournament\n",
    "    all_tournament_results_dict = multi_tournament_wr_per_tournament(scrape_results_dict)\n",
    "\n",
    "    # Calculate winrates\n",
    "    multi_tournament_wr_calc(all_tournament_results_dict)\n",
    "    \n",
    "    # Create plot_df\n",
    "    plot_df = create_plot_df(all_tournament_results_dict)\n",
    "    \n",
    "    # 8. Append plot_df to current results, and update checkpoint\n",
    "    update_results(current_results_df, plot_df)\n",
    "    update_checkpoint(all_tournament_results_dict, ckpt_df)\n",
    "                           \n",
    "else: \n",
    "    logging.info(\"Checkpoint not in use. Scraping all data...\")\n",
    "    # Scrape everything\n",
    "    url_list = df_latenight['URL'].unique().tolist()\n",
    "    url_dict = create_urls(url_list)\n",
    "\n",
    "    scrape_results_dict = multi_latenight_scrape(url_dict)\n",
    "    scrape_results_dict = add_date_to_dict(scrape_results_dict, df_latenight)\n",
    "    \n",
    "    # Process data: Get WLT counts for each deck in each tournament\n",
    "    all_tournament_results_dict = multi_tournament_wr_per_tournament(scrape_results_dict)\n",
    "\n",
    "    # Calculate winrates\n",
    "    multi_tournament_wr_calc(all_tournament_results_dict)\n",
    "    \n",
    "    # Create plot_df\n",
    "    plot_df = create_plot_df(all_tournament_results_dict)\n",
    "    \n",
    "    # Define variables and paths\n",
    "    today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    path_to_latest = os.path.join(os.getcwd(), \"results/latest/scrape_results.csv\")\n",
    "    path_to_dated = os.path.join(os.getcwd(), f\"results/dated/scrape_results_{today}.csv\")\n",
    "\n",
    "    # Save results to latest and dated\n",
    "    logging.info(\"Saving results to latest...\")\n",
    "    plot_df.to_csv(path_to_latest, header=True, index=False)\n",
    "    logging.info(\"Saving results to dated...\")\n",
    "    plot_df.to_csv(path_to_dated, header=True, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb02979-c1d3-412b-9274-087384999c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
